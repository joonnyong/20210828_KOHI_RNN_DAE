{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Denoising photoplethysmogram (PPG) data using autoencoder model with ANN, RNN, or BiRNN\n",
    "\n",
    "    1) data loading using a 40Hz 10sec PPG dataset extracted from various sources\n",
    "    - during data loading, noises can be added to the data to train the denoising model\n",
    "\n",
    "    2) definitions of models: ANN, RNN, BiRNN\n",
    "\n",
    "    3) compiling the denoising autoencoder model based on (2), training, and validation\n",
    "\n",
    "    Tensorflow-Keras 2.3.0\n",
    "\n",
    "    Author: Joonnyong Lee, PhD (CEO, Mellowing Factory Co., Ltd.)\n",
    "    Date: 2020-8-31\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Create a folder called '20200917_KOHI_RNN_DAE-master' in your Google Drive and upload this file and the unzipped PPG data file\"\"\"\n",
    "\n",
    "\"\"\" 런타임 > 런타임 유형 변경 (GPU) \"\"\"\n",
    "\"\"\" Install \"\"\"\n",
    "# !pip install tensorflow-gpu\n",
    "\n",
    "# load the required libraries\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# which deep learning model to use\n",
    "model_choice = 'ANN'  # 'RNN' or 'BiRNN'\n",
    "\n",
    "# model parameters\n",
    "timesteps = 60\n",
    "sampling_frequency = 40\n",
    "hidden_layer_units = 10\n",
    "\n",
    "# create checkpoint path for the model\n",
    "if model_choice == 'ANN':\n",
    "    FILEWRITER_PATH = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_DAE_ANN_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'nodes_tensorboard'\n",
    "    if not os.path.isdir(FILEWRITER_PATH):\n",
    "        os.makedirs(FILEWRITER_PATH)\n",
    "    CHECKPOINT_PATH = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_DAE_ANN_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'nodes_tensorboard/checkpoints'\n",
    "    if not os.path.isdir(CHECKPOINT_PATH):\n",
    "        os.makedirs(CHECKPOINT_PATH)\n",
    "elif model_choice == 'RNN':\n",
    "    FILEWRITER_PATH = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_DAE_RNN_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'nodes_tensorboard'\n",
    "    if not os.path.isdir(FILEWRITER_PATH):\n",
    "        os.makedirs(FILEWRITER_PATH)\n",
    "    CHECKPOINT_PATH = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_DAE_RNN_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'nodes_tensorboard/checkpoints'\n",
    "    if not os.path.isdir(CHECKPOINT_PATH):\n",
    "        os.makedirs(CHECKPOINT_PATH)\n",
    "elif model_choice == 'BiRNN':\n",
    "    FILEWRITER_PATH = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_DAE_BiRNN_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'nodes_tensorboard'\n",
    "    if not os.path.isdir(FILEWRITER_PATH):\n",
    "        os.makedirs(FILEWRITER_PATH)\n",
    "    CHECKPOINT_PATH = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_DAE_BiRNN_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'nodes_tensorboard/checkpoints'\n",
    "    if not os.path.isdir(CHECKPOINT_PATH):\n",
    "        os.makedirs(CHECKPOINT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------- load data & pre-processing -------------------------------------------- #\n",
    "\n",
    "# set this to the path of your data file\n",
    "PPG_data_path = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_40Hz_10sec_25000set/PPG_40Hz_10sec_25000set.txt'\n",
    "print(PPG_data_path)\n",
    "\n",
    "def signal_extraction(data_path):\n",
    "\n",
    "    # load the dataset\n",
    "    # the [0] index is there because there is one file in this dataset, if there are more than file for the dataset, do a for-loop to load each data file\n",
    "    PPG_data = np.loadtxt(data_path)\n",
    "    print('The shape of the original dataset is %i by %i' % np.shape(PPG_data))  # (10000, 400) -> 10000 rows of 40Hz 10sec PPG data\n",
    "    print('The shape of the %i timestep dataset is %i by %i' % (timesteps, int(np.ceil((400-timesteps)/timesteps)*len(PPG_data)), timesteps))\n",
    "\n",
    "    # shuffle the data\n",
    "    shuffle(PPG_data)\n",
    "\n",
    "    # create empty lists to hold training and validation datasets\n",
    "    train_input_data_list = []\n",
    "    train_output_data_list = []\n",
    "    val_input_data_list = []\n",
    "    val_output_data_list = []\n",
    "\n",
    "    # loop through the original dataset and perform data extraction/noise addition on each row\n",
    "    for datanum in range(len(PPG_data)):\n",
    "        # separate the PPG segment in each row into timestep size\n",
    "        for index in range(0, 400-timesteps, timesteps):\n",
    "\n",
    "            # ---------- original data, normalized ----------\n",
    "            a = PPG_data[datanum, index:index + timesteps].copy()\n",
    "            a = a - np.min(a)\n",
    "            a = a / np.max(a)\n",
    "\n",
    "            # ---------- Gaussian noise data ----------\n",
    "            noise1 = ???????????????????????????\n",
    "            b = a.copy() + noise1\n",
    "            b = b - np.min(b)\n",
    "            b = b / np.max(b)\n",
    "\n",
    "            # ---------- Gaussian noise + low freq noise data ----------\n",
    "            c = b.copy()\n",
    "            slope = ?????????\n",
    "            for i in range(len(c)):\n",
    "                c[i] = c[i] + ???????????????????\n",
    "            c = c - np.min(c)\n",
    "            c = c / np.max(c)\n",
    "\n",
    "            # ---------- Gaussian noise + low freq noise + saturation data ----------\n",
    "            d = c.copy()\n",
    "            location1 = ??????????????????????????\n",
    "            location2 = location1 + ????????????????\n",
    "            d[location1:location2] = ??????????\n",
    "            \n",
    "\n",
    "            # put 80% of the data into the training dataset list, and 20% into the validation dataset list\n",
    "            if datanum < 0.8 * len(PPG_data):\n",
    "                train_input_data_list.append(a)  # CHANGE THIS TO THE NOISE-AUGMENTED DATA\n",
    "                train_output_data_list.append(a)\n",
    "            else:\n",
    "                val_input_data_list.append(a)  # CHANGE THIS TO THE NOISE-AUGMENTED DATA\n",
    "                val_output_data_list.append(a)\n",
    "\n",
    "    # plot the last data\n",
    "    plt.figure()\n",
    "    plt.plot(PPG_data[datanum])\n",
    "    plt.plot(PPG_data[datanum,index:index + timesteps])\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(a)\n",
    "    plt.plot(b)\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(a)\n",
    "    plt.plot(c)\n",
    "    plt.show()\n",
    "    plt.figure()\n",
    "    plt.plot(a)\n",
    "    plt.plot(d)\n",
    "    plt.show()\n",
    "\n",
    "    # convert the dataset lists into arrays\n",
    "    train_input_data_list = np.asarray(train_input_data_list)\n",
    "    train_output_data_list = np.asarray(train_output_data_list)\n",
    "    val_input_data_list = np.asarray(val_input_data_list)\n",
    "    val_output_data_list = np.asarray(val_output_data_list)\n",
    "\n",
    "    return train_input_data_list, train_output_data_list, val_input_data_list, val_output_data_list\n",
    "\n",
    "\n",
    "[train_input_data_list, train_answer_data_list, val_input_data_list, val_answer_data_list] = signal_extraction(PPG_data_path)\n",
    "\n",
    "# if the deep learning model used here is recurrent, need to reshape the input data to match the keras LSTM input_shape\n",
    "if model_choice == 'RNN' or model_choice == 'BiRNN':\n",
    "    train_input_data_list = np.reshape(train_input_data_list, [len(train_input_data_list), timesteps, 1])\n",
    "    val_input_data_list = np.reshape(val_input_data_list, [len(val_input_data_list), timesteps, 1])\n",
    "\n",
    "print(np.shape(train_input_data_list))\n",
    "print(np.shape(train_answer_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------ deep learning model definition ------------------------------------------ #\n",
    "\n",
    "def ANN():\n",
    "\n",
    "    model_inputs = tf.keras.Input(shape=(timesteps, ))\n",
    "    encoding_layer = tf.keras.layers.Dense(units=hidden_layer_units, activation='sigmoid', input_shape=(timesteps,))(model_inputs)\n",
    "    out_layer = ??????????????????????????\n",
    "\n",
    "    model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def RNN():\n",
    "\n",
    "    model_inputs = tf.keras.Input(shape=(timesteps, 1))\n",
    "    encoding_layer = ?????????????????\n",
    "    out_layer = ?????????????\n",
    "\n",
    "    model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def BiRNN():\n",
    "\n",
    "    model_inputs = tf.keras.Input(shape=(timesteps, 1))\n",
    "    encoding_layer = ??????????????\n",
    "    out_layer = ???????????????????\n",
    "\n",
    "    model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "def BiRNN_MM():\n",
    "\n",
    "    model_inputs = tf.keras.Input(shape=(timesteps, 1))\n",
    "    encoding_layer = ????????????????\n",
    "    out_layer = ??????????????\n",
    "\n",
    "    model = tf.keras.Model(inputs=model_inputs, outputs=out_layer)\n",
    "\n",
    "    return model\n",
    "\n",
    "print('Setting model complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- deep learning model training & validation -------------------------------------- #\n",
    "\n",
    "if model_choice == 'ANN':\n",
    "    DAE = ANN()\n",
    "elif model_choice == 'RNN':\n",
    "    DAE = RNN()\n",
    "elif model_choice == 'BiRNN':\n",
    "    DAE = BiRNN()\n",
    "\n",
    "# compile the model with optimizer and loss function\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=1)\n",
    "DAE.compile(optimizer='adam', loss='mse')\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(CHECKPOINT_PATH, verbose=1, save_best_only=True, save_weights_only=True, period=1)\n",
    "\n",
    "# train the model\n",
    "DAE.fit(train_input_data_list, train_answer_data_list, epochs=10000, batch_size=40000, shuffle=True, validation_data=(val_input_data_list, val_answer_data_list), verbose=2, callbacks=[cp_callback])\n",
    "\n",
    "# load the trained model\n",
    "DAE.load_weights(CHECKPOINT_PATH)\n",
    "\n",
    "# generate predictions (inferences) based on validation input data\n",
    "prediction = DAE.predict(val_input_data_list)\n",
    "\n",
    "# reshape the validation input data, prediction, and the answer for saving purposes\n",
    "prediction = np.reshape(prediction, [-1])\n",
    "val_input_data_list = np.reshape(val_input_data_list, [-1])\n",
    "val_answer_data_list = np.reshape(val_answer_data_list, [-1])\n",
    "\n",
    "# save 10% of the validation results for further analyses\n",
    "name = '/content/gdrive/My Drive/20200917_KOHI_RNN_DAE-master/PPG_DAE_' + model_choice + '_' + str(timesteps) + 'timesteps_' + str(hidden_layer_units) + 'nodes_prediction_results.txt'\n",
    "file = open(name, 'w')\n",
    "for result_num in range(int(len(prediction)/10)):\n",
    "    file.write(\"%f %f %f \\n\" % (val_input_data_list[result_num], prediction[result_num], val_answer_data_list[result_num]))\n",
    "file.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_input_data_list[0:timesteps])\n",
    "plt.plot(prediction[0:timesteps])\n",
    "plt.plot(val_answer_data_list[0:timesteps])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
